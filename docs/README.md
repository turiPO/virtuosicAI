[TOC]
# Abstract
In this work we examine the ability of a machine to compose good quality music with minimalistic amount of resources. Beginning with relevant review of previous work and the current state of the art model we describe new approache that hasn't been done yet in the music geenration field.

# Music Generation - A Review
## Passing the Turing test

Research on AI-based music composition has been ongoing since 1980. Various classic algorithms and methods from information theory, probabilistic models, chaos theory, and genetic algorithms have been employed for this task \[[1]\].

Recently, there has been a surge in the use of deep learning techniques for music generation. This can be attributed to the success of these techniques in other domains such as computer vision and natural language processing. Deep learning provides a powerful tool for learning complex patterns in data. The first instance of music generated by deep learning passing the Turing test was the "Deep Bach" project in 2017, which composed chorales based on Bach chorales.

The Deep Bach project combined classic algorithms with a Bidirectional Recurrent Neural Network (RNN) model that was trained using masking-prediction erasure of arpeggios in Bach chorales. Additionally, this work introduced two revolutionary approaches:

Similar to image augmentation, the researchers expanded the training dataset by augmenting the music scale of each chorale.
The depth of the RNN was determined based on the average radius between chords and their dependents \[[2]\].

In parallel, the Magenta project (acquired by Google) worked on a similar task and published their findings in 2019. Instead of using recurrent neural networks (RNNs), they employed Convolutional Neural Networks (CNNs) to complete partial scores. To address the chronologicalization issue inherent in CNNs, the generated scores were passed through a machine learning model that assessed the likelihood of the output based on previous chords and "fixed" it to be more chronological. Compared to RNNs, which can suffer from vanishing or exploding gradients due to their short memory, this CNN-based method is more efficient as it can run in parallel and has proven capable of capturing both local and large-scale structure \[[3]\].

Despite these advancements, composing chorales is considered an "easy task" due to their highly patterned nature and adherence to strict rules. Chorales are music pieces without melody or rhythm, consisting of four arpeggios in each bar, limited to a four-voice choir, and composed under the strict harmony rules of the Baroque era. In fact, when Bach deviated from these harmony rules, he was heavily criticized in the newspapers.

## Advanced models
The primary challenge in music generation lies in generating longer and higher-quality music pieces across different genres. Thanks to recent revolutions in natural language processing (NLP) and computer vision, similar progress is being made in the field of music generation. The major breakthroughs in music generation over the past few years can be categorized into two approaches: latent-based models and language-based models. Latent-based models include VAEs (e.g., MusicVAE, PianoTree, Jukebox), while successful models based on GANs (e.g., GANSynth, jazzGAN) also exist. On the other hand, music has traditionally been viewed as a language, leading to state-of-the-art solutions based on transformers (e.g., Music Transformer, MuseNet).

MusicVAE was the first VAE to successfully capture "long-term structure" within 16 bars, enabling it to generate diverse styles of melodies with good quality. Unlike vision-related works with VAEs, MusicVAE employs a Bidirectional RNN as both the decoder and encoder to enable the choice of piece length. It was trained on the Lakh MIDI Dataset and an additional 1.5 million MIDI files obtained from the web, without performance information. Furthermore, MusicVAE achieves favorable results when interpolating the latent space between two pieces \[[4]\].

Transformers were initially introduced for NLP and proved to be highly successful. The key factor behind their success was the introduction of the attention layer, which effectively learned the relationships between words in a sentence. Since the attention layer is oblivious to word order, the paper's authors devised a method to encode the positional information of each word. Unlike RNNs, transformers can be trained with large amounts of data, which have become more accessible due to the development of the internet, allowing unsupervised training for NLP tasks \[[5]\].

The Magenta team was the first to apply the transformer revolution to the field of music with "Music Transformer." Since each "music sentence" is longer than a typical natural language sentence, the paper proposed an alternative attention layer algorithm that reduces the space complexity from quadratic to linear. Music Transformer was trained on the traditional YAMAHA piano-e-competition dataset, as well as all the piano videos available on YouTube \[[6]\].

OpenAI has also developed two transformer-based models: Musenet and Jukebox. Musenet shares a similar architecture to GPT-2 and is trained using a masking approach, predicting the next token in a sequence. It was trained on MIDI datasets augmented for improved results and expanded training data volume \[[7]\].

Jukebox represents the current state-of-the-art music generator capable of generating melody, harmony, lyrics, background noises, and human sounds. The Jukebox dataset comprises 1.2 million songs crawled from the web, complete with lyrics and song metadata. Compared to most of the models discussed in this paper, Jukebox stands out for its comprehensive capabilities \[[8]\].

<!---
## Other Developments

While automatic music generation 

Short pieces:
MusicVAE - https://magenta.tensorflow.org/music-vae
Music Trasformer - magenta
Musenet - GPT2 openai 

Longer pieces:
TransformerVAE
PianoTree
DDPM
Try to find structure with C-RBM.

## Generate melody given harmony 
VAE - Generating nontrivial melodies for music as a service 2017
[https://arxiv.org/pdf/1809.07600.pdf]
JazzGAN - Improvising with generative adversarial networks 2018
[https://musicalmetacreation.org/mume2018/proceedings/Trieu.pdf]
BebopNet - jazz improvisiations with LSTM 2020
[https://program.ismir2020.net/static/final_papers/132.pdf]

## Musical style transfer
Although the lack of datasets style transfer methods has been used such as tune transfer and instruments addition algorithms, VAE and transfer learning. [https://arxiv.org/pdf/2108.12290.pdf]
The first DNN which succeeded to transfer style of a complete music piece was MIDI-VAE in 2018.   [https://arxiv.org/pdf/1809.07600.pdf]
Transfer learning methods found as very effective for this task, <> find tuned pop generation model into urban music. [????]

Another effective method was by learning the PianoTree VAE model latent vector representation and modify it in a way which changes the style of the music. [https://arxiv.org/pdf/2008.07122.pdf]

GAN based method to mix many genres [https://arxiv.org/pdf/1712.01456.pdf]

## Challenges in music generation

### Evaluation of  the music

The music generation task is inherently unsupervised learning task.

* Brain EEG (technion jazz paper)
* chord progression histogram
* Train validators (for style transfer)
* Listening tests (like in MuseicVAE)
* creativity
  * Rote Memorization frequencies (RM): Given a specified
  length l, RM measures how frequently the model copies
  note sequences of length l from the corpus. [https://musicalmetacreation.org/mume2018/proceedings/Trieu.pdf]
  * Pitch variation [https://musicalmetacreation.org/mume2018/proceedings/Trieu.pdf]

## Feature Engineering

A music piece contains a few important elements, which used as feature in the generation papaers:

1. Key - which can be switched between parts of the piece
2. Rhythm
3. Structure - Sonata, perlude, ABABA @todo
4. Melody @todo
5. Texture - @todo
6. Instruments - voice amount, voices range, what kind of instrument etc.
-->
# First topic: prior latent space optimization using MusicVAE 
## Introduction
Optimizing a model is a crucial task in the field of Deep Learning (DL), whether it is to enhance the quality of the output or to adapt the model to perform a more specific task. In the context of music generation, numerous models can generate music of good quality, but they often lack the ability to produce music of a specific genre.

One of the primary challenges with latent-based music generation models is the unpredictable behavior of the latent space. Frequently, the generated output exhibits a mixture of styles from the training data, or a randomly selected style that only loosely represents the training data. Moreover, fine-tuning the entire network for each style or individual piece reconstruction is highly inefficient and unfeasible when working with limited resources.

In the domain of computer vision, researchers have addressed a similar issue by utilizing less expressive neural networks that study the distribution of the latent space through KL-divergence loss, enabling generative models to be conditional. For instance, StyleGAN can reconstruct a specific face or interpolate between faces while setting a particular hair color \[[9]\] \[[10]\].

## Approach 
In this study, we employ a latent space optimization technique using the MusicVAE model to enable conditioning based on a prior. To achieve this, we freeze the weights of the VAE decoder and train a new, smaller version of the encoder, following the training methodology outlined in the original paper. Since the training data consists of specific genres of MIDI files, the new encoder can be significantly smaller than the one described in the original MusicVAE paper. Although this approach has been tested on MusicVAE, it can be adapted for any latent-based music generation model.

## Implementation
We have developed the capability to fine-tune any part of the MusicVAE 16-bar generation model, allowing it to be trained with a focus on a specific genre, thereby introducing a genre bias. For research purposes, we provide several options: training only the decoder, training only the encoder, or training both components. It is important to note that creating a smaller encoder is not necessary for MusicVAE, as the training process is sufficiently fast, but it may be explored in future work.

[code: https://github.com/turiPO/virtuousicAI/tree/main/PriorMusicVAE]

## Future work
In the field of music generation, there are several crucial areas that warrant further exploration. Addressing the lack of comprehensive datasets and developing robust evaluation methods are key priorities to enhance the quality and variety of generated music. Additionally, extending the latent space optimization approach to other models would provide fine-grained control over music generation, enabling specific genre or style biases. Moreover, an intriguing avenue is the development of a text-to-music generative model based on the research conducted in this study, leveraging insights from latent space optimization and conditioning techniques to translate textual input into coherent and expressive musical compositions. By advancing in these directions, we can unlock new possibilities for creative expression and push the boundaries of generative music.

# Second topic: copyright of all the future song lyrics
## Abstract
The exponential growth of storage and computing power over the past decade has posed challenges for copyright laws to keep pace with advancing technology. This paper addresses the issue of copyrightability of future song lyrics and accidental infringement of pop song lyrics. The potential exploitation of copyright laws by law firms to sue for royalties based on computing common future pop song lyrics is discussed. To prevent this, the paper proposes copyrighting all likely future song lyrics and thereby mitigating accidental infringement of existing pop songs.

## Copyright laws issue in the pop music industry
Copyright is a critical subject within the art industry. While song titles, rhythms, musical styles, and harmony are generally not copyrightable, melodies and lyrics enjoy legal protection. Originally, copyright laws aimed to safeguard artists who created easily reproducible artworks such as paintings, music compositions, and literature. However, in recent years, musicologists and anthropologists have highlighted the industrialization and commercialization of pop music. These researchers argue that compared to classical or jazz genres, pop songs tend to have less complex melodies, harmonies, and lyrics, with greater similarity among them. Conversely, other researchers assert that pop music complexity has shifted to aspects like music visual clips and sound effects, rendering traditional copyright laws inadequate in protecting pop music. \[[11]\]

## Related work
The issue of copyrighting melodies has been explored in a TED talk by Daniel Riehl. Riehl's approach involves brute-forcing all future common melodies of pop songs and storing them, constituting valid copyright claims for those melodies. By providing public access to these copyrights, Riehl aims to prevent accidental infringement.  \[[12]\]

## Approach
Utilizing a corpus of pop song lyrics spanning from 1950 to 2020, a Markov chain model is created to represent the probability of word transitions in pop songs. This model enables the computation of common combinations with a defined probability threshold, allowing estimation of the computational and storage requirements necessary to store and copyright all likely future lyrics.

## Implemetation
One of the challenges encountered was the scarcity of a pure English pop lyrics dataset. While most available datasets encompassed other genres and languages, a dataset primarily comprising English pop songs was selected for proof-of-concept purposes. Preprocessing techniques were applied to this dataset. \[[13]\] 
Subsequently, the dataset was used to train a maximum likelihood estimation (MLE)-based language model, specifically a simple Markov chain. This model tallies the frequency of word transitions. After training on the entire dataset, the number of common combinations exceeding a defined probability threshold is computed. Estimation of the required storage and computational power can be derived from the number of combinations and the storage capacity needed for each combination.

[code: https://github.com/turiPO/virtuosicAI/tree/main/CopyrightFuturePop]

## Future work
In a hypothetical scenario where all future melodies and lyrics can be stored on a disk, the relevance of copyright law becomes questionable. However, it is highly improbable that the current legal interpretation would align with such a notion. An alternative solution could involve developing a tool to assess the distinctiveness of a song in comparison to previously published pop songs. Such a tool could be utilized by record companies and artists to determine if a song meets the originality threshold for publication. Furthermore, the implementation of the Markov chain can be enhanced by leveraging more accurate neural language models.


# Reference
\[1\] "Gaëtan Hadjeres, François Pachet, Frank Nielsen. Carlos Hernandez-Olivan, Jose R. Beltran. Music Composition with Deep Learning: A Review, 2017." <br>
\[2\]  " DeepBach: a Steerable Model for Bach Chorales Generation, 2021." <br>
\[3\] "Cheng-Zhi Anna Huang, Tim Cooijmans, Adam Roberts, Aaron Courville, Douglas Eck. Counterpoint by Convolution, 2019." <br>
\[4\] "Adam Roberts, Jesse Engel, Colin Raffel, Curtis Hawthorne, Douglas Eck. A Hierarchical Latent Vector Model for Learning Long-Term Structure in Music, 2019." <br>
\[5\] "Vaswani, Ashish, Shazeer, Noam, Parmar, Niki, Uszkoreit, Jakob, Jones, Llion, Gomez, Aidan N, Kaiser, Łukasz and Polosukhin, Illia. Attention is all you need, 2017." <br>
\[6\] "Cheng-Zhi Anna Huang, Ashish Vaswani, Jakob Uszkoreit, Noam Shazeer. Music Transformer, 2018" <br>
\[7\] "Christine. MuseNet. OpenAI, 2019." <br>
\[8\] "Prafulla Dhariwal, Heewoo Jun, Christine Payne, Jong Wook Kim, Alec Radford, Ilya Sutskever. Jukebox: A Generative Model for Music, 2020." <br>
\[9\] "Aviv Gabbay, Yedid Hoshen. Style Generator Inversion for Image Enhancement and Animation, 2019." <br>
\[10\] "Piotr Bojanowski, Armand Joulin, David Lopez-Paz, Arthur Szlam. Optimizing the Latent Space of Generative Networks, 2018." <br>
\[11\]: "Adorno, Theodor W. and Horkheimer, Max. The Culture Industry: Enlightenment as Mass Deception. West Sussex: Columbia University Press, 2020, pp. 80-96" <br>
\[12\]: "Damien Riehl. Copyrighting all the melodies to avoid accidental infringement. 2020" <br>
\[13\]: "https://github.com/mathigatti/pop-lyrics-dataset/tree/master/lyrics" <br>

[1]: <https://doi.org/10.48550/arXiv.2108.12290> "Gaëtan Hadjeres, François Pachet, Frank Nielsen. Carlos Hernandez-Olivan, Jose R. Beltran. Music Composition with Deep Learning: A Review, 2017."
[2]: <https://doi.org/10.48550/arXiv.1612.01010> " DeepBach: a Steerable Model for Bach Chorales Generation, 2021."
[3]: <https://doi.org/10.48550/arXiv.1903.07227> "Cheng-Zhi Anna Huang, Tim Cooijmans, Adam Roberts, Aaron Courville, Douglas Eck. Counterpoint by Convolution, 2019."
[4]: <https://doi.org/10.48550/arXiv.1803.05428> "Adam Roberts, Jesse Engel, Colin Raffel, Curtis Hawthorne, Douglas Eck. A Hierarchical Latent Vector Model for Learning Long-Term Structure in Music, 2019."
[5]: <https://doi.org/10.48550/arXiv.1706.03762> "Vaswani, Ashish, Shazeer, Noam, Parmar, Niki, Uszkoreit, Jakob, Jones, Llion, Gomez, Aidan N, Kaiser, Łukasz and Polosukhin, Illia. Attention is all you need, 2017."
[6]: <https://doi.org/10.48550/arXiv.1809.04281> "Cheng-Zhi Anna Huang, Ashish Vaswani, Jakob Uszkoreit, Noam Shazeer. Music Transformer, 2018"
[7]: <https://openai.com/blog/musenet/> "Payne, Christine. MuseNet. OpenAI, 2019."
[8]: <https://doi.org/10.48550/arXiv.2005.00341> "Prafulla Dhariwal, Heewoo Jun, Christine Payne, Jong Wook Kim, Alec Radford, Ilya Sutskever. Jukebox: A Generative Model for Music, 2020."
[9]: <https://doi.org/10.48550/arXiv.1906.11880> "Aviv Gabbay, Yedid Hoshen. Style Generator Inversion for Image Enhancement and Animation, 2019."
[10]: <https://doi.org/10.48550/arXiv.1707.05776> "Piotr Bojanowski, Armand Joulin, David Lopez-Paz, Arthur Szlam. Optimizing the Latent Space of Generative Networks, 2018."
[11]: <https://doi.org/10.7312/kul-17602-005> "Adorno, Theodor W. and Horkheimer, Max. The Culture Industry: Enlightenment as Mass Deception. West Sussex: Columbia University Press, 2020, pp. 80-96"
[12]: <https://www.youtube.com/watch?v=sJtm0MoOgiU> "Damien Riehl. Copyrighting all the melodies to avoid accidental infringement. 2020"
[13]: <https://github.com/mathigatti/pop-lyrics-dataset/tree/master/lyrics> "https://github.com/mathigatti/pop-lyrics-dataset/tree/master/lyrics"

# Appendix - small experiments
During the work we conducted many experiment with different models and datasets. Here are some of them:
*Music Transformer longer melodies:* Music transformer is a large transformer model by Google Magenta which can produce melodies using a "prompt" - beginning of the melody. One of the main issues with Music Transformer is that it is very limited by the context it can preserved through time, means after a few minutes the music sounds like random. In order to overcome this limitation we tried to concat melodies by taking each melody ending as the prompt for the next one. The results where far better than the naive generation of long melody.
[code: https://github.com/turiPO/virtuosicAI/blob/main/notebooks/music%20transfomer.ipynb]
<br>
*MusicVAE with conditional varience:* Dropping the encoder of the VAE, we can inject a random vector into the decoder and create melodies. A regular VAE interpret the encoder output as mean and varience for normal gaussian sampling, which then feed the output of it into the decoder. In order to test MusicVAE we sampled from a normal distribution and injected it into the decoder. Each time we sampled with higher varience in order to validate the behaviour of the model. The results where as expected, the model generated more random melodies with higher varience and total random when the vector was randomed. 
[code: https://github.com/turiPO/virtuosicAI/blob/main/notebooks/MusicVAE%20notebooks.ipynb]
