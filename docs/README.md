[TOC]
# Abstract
In this work we examine the ability of a machine to compose good quality music with minimalistic amount of resources. Beginning with relevant review of previous work and the current state of the art model we describe new approache that hasn't been done yet in the music geenration field.

# Music Generation - A Review
## Passing the Turing test

AI based music composition has been in research since 1980. Classic algorithms, methods from information theory, probabilistic models, chaos theory and genetics algorithms were mainly in use for this task. \[[1]\]

There has been a recent surge in the use of deep learning techniques for music generation. This is largely due to the success of these techniques in other domains such as computer vision and natural language processing. Deep learning offers a powerful tool for learning complex patterns in data. The first time music generated by deep learning (DL) passed the Turing test was the "Deep Bach" project in 2017, were they composed chorales music based on Bach chorales.

The Deep Bach project mixed classic algorithms as well Bidirectional recurrent neural network (RNN) model which has been trained by masking - prediction erased arpeggios in Bach chorales (randomly erased arpeggio and ask the model to reconstruct it). In addition, this work was revolutionised in two ways:
1. Similar to image augmentation, the researchers expanded the training dataset by augmenting the music scale of each chorale.
2. Chose the depth of the RNN according the average radius between chord and it's dependents.
\[[2]\]

Meanwhile Magenta project (Acquired by Google) worked on a similar task and publish in 2019. Instead of using recurrent NN (neural network) they used CNN (convolutional NN) to complete partial scores. In order to fix CNN lack of chronologicalization, after the generation the scores run by ML model which check the likelihood of the output based on previous chords and "fix" it to be more chronological. While RNN limited by it's short memory (vanishing/exploding gradient), this CNN based method is more efficient because it can run parallel and proved to be able to capture local structure as well as large scale structure. \[[3]\]

Despite the improvement,  chorales composition is an "easy task" because they are very patterned and been written under strict rules. Chorale is a music piece without melody or rhythm which contains four arpeggios in each bar, limited to four voices choir and written under strict harmony rules of the baroque era. Actually, when Bach didn't follow the harmony rules he had been severely criticised in the newspapers.

## Advanced models
The main challenge in the music generation task is to generate longer and better quality music pieces within different genres and thanks to the recent revolutions in NLP and computer vision it's started to happen also in the music field. Most of the music generation breakthroughs of the past years can be divide into two approaches: latent based models and language based model. Latent based models papers are based mainly on VAEs (e.g. MusicVAE, PianoTree, Jukebox), although there are quite successful ones with GANs (e.g. GANSynth, jazzGAN). On the contrasts, music traditionally has been though as a language model, and thus there are state of the art solutions based on transformers (e.g. Music Transformer, Musenet).

The first time VAE has succeeded to catch "long term structure" within 16 bars was MusicVAE, which is able to generate style diverse and good quality melodies. Unlike Vision works with VAE, in order to be able to choose piece length MusicVAE uses Bi-RNN as the decoder and encoder. MusicVAE trained on the Lakh MIDI Dataset and another 1.5 million MIDIs on the web without performance information. In addition, MusicVAE achieves good results when interpolates the latent space between two pieces. \[[4]\] 

Transformers based model were first introduced for NLP and were very successful. The main reason for the success was the introduction of the attention layer, which was able to learn the right relation between words in the sentence. Attention layer is ignorant to the order of the words in the sentence, therefore the writers of the paper found a way to encode the position of each word. Unlike RNNs, transformers can be trained with big amount of data, which is more accessible due to the development of the internet and managed to be trained unsupervised for NLP tasks. \[[5]\]

Magenta team were the first ones to adopt the transformers revolution into the music world  with "Music Transformer". Each "music sentence" is longer than natural language sentence, therefore the paper writers suggested an alternative attention layer algorithm which reduces the space complexity from squared into linear. Music Transformer has been trained on the traditional YAMAHA piano-e-competition dataset as well as all the Youtube piano videos. \[[6]\].

OpenAI created two transformer based models as well: first Musenet and then Jukebox. Musnet has similar architecture as GPT-2 and trained with masking as well: prediction of a next token in a sequence. Musenet has been trained on MIDIs datasets which where augmented for better results and for expansion of the amount of the training data. \[[7]\]

Jukebox is the current state of the art music generator, it can generate both melody and harmony and performance elements like lyrics, background noises and human sound. Jukebox dataset contains 1.2 milion songs which was crawled from the web with lyrics and song metadata. In comparison to most of the models which has been discussed in this paper, Jukebox is . \[[8]\]

<!---
## Other Developments

While automatic music generation 

Short pieces:
MusicVAE - https://magenta.tensorflow.org/music-vae
Music Trasformer - magenta
Musenet - GPT2 openai 

Longer pieces:
TransformerVAE
PianoTree
DDPM
Try to find structure with C-RBM.

## Generate melody given harmony 
VAE - Generating nontrivial melodies for music as a service 2017
[https://arxiv.org/pdf/1809.07600.pdf]
JazzGAN - Improvising with generative adversarial networks 2018
[https://musicalmetacreation.org/mume2018/proceedings/Trieu.pdf]
BebopNet - jazz improvisiations with LSTM 2020
[https://program.ismir2020.net/static/final_papers/132.pdf]

## Musical style transfer
Although the lack of datasets style transfer methods has been used such as tune transfer and instruments addition algorithms, VAE and transfer learning. [https://arxiv.org/pdf/2108.12290.pdf]
The first DNN which succeeded to transfer style of a complete music piece was MIDI-VAE in 2018.   [https://arxiv.org/pdf/1809.07600.pdf]
Transfer learning methods found as very effective for this task, <> find tuned pop generation model into urban music. [????]

Another effective method was by learning the PianoTree VAE model latent vector representation and modify it in a way which changes the style of the music. [https://arxiv.org/pdf/2008.07122.pdf]

GAN based method to mix many genres [https://arxiv.org/pdf/1712.01456.pdf]

## Challenges in music generation

### Evaluation of  the music

The music generation task is inherently unsupervised learning task.

* Brain EEG (technion jazz paper)
* chord progression histogram
* Train validators (for style transfer)
* Listening tests (like in MuseicVAE)
* creativity
  * Rote Memorization frequencies (RM): Given a specified
  length l, RM measures how frequently the model copies
  note sequences of length l from the corpus. [https://musicalmetacreation.org/mume2018/proceedings/Trieu.pdf]
  * Pitch variation [https://musicalmetacreation.org/mume2018/proceedings/Trieu.pdf]

## Feature Engineering

A music piece contains a few important elements, which used as feature in the generation papaers:

1. Key - which can be switched between parts of the piece
2. Rhythm
3. Structure - Sonata, perlude, ABABA @todo
4. Melody @todo
5. Texture - @todo
6. Instruments - voice amount, voices range, what kind of instrument etc.
-->
# Research no.1: prior latent space optimization using MusicVAE 
## Introduction
Optimizing a model is a crucial task in the field of Deep Learning (DL), whether it is to enhance the quality of the output or to adapt the model to perform a more specific task. In the context of music generation, numerous models can generate music of good quality, but they often lack the ability to produce music of a specific genre.

One of the primary challenges with latent-based music generation models is the unpredictable behavior of the latent space. Frequently, the generated output exhibits a mixture of styles from the training data, or a randomly selected style that only loosely represents the training data. Moreover, fine-tuning the entire network for each style or individual piece reconstruction is highly inefficient and unfeasible when working with limited resources.

In the domain of computer vision, researchers have addressed a similar issue by utilizing less expressive neural networks that study the distribution of the latent space through KL-divergence loss, enabling generative models to be conditional. For instance, StyleGAN can reconstruct a specific face or interpolate between faces while setting a particular hair color \[[9]\] \[[10]\].

## Approach 
In this study, we employ a latent space optimization technique using the MusicVAE model to enable conditioning based on a prior. To achieve this, we freeze the weights of the VAE decoder and train a new, smaller version of the encoder, following the training methodology outlined in the original paper. Since the training data consists of specific genres of MIDI files, the new encoder can be significantly smaller than the one described in the original MusicVAE paper. Although this approach has been tested on MusicVAE, it can be adapted for any latent-based music generation model.

## Implementation
We have developed the capability to fine-tune any part of the MusicVAE 16-bar generation model, allowing it to be trained with a focus on a specific genre, thereby introducing a genre bias. For research purposes, we provide several options: training only the decoder, training only the encoder, or training both components. It is important to note that creating a smaller encoder is not necessary for MusicVAE, as the training process is sufficiently fast, but it may be explored in future work.

[code: https://github.com/turiPO/virtuousicAI/tree/main/PriorMusicVAE]

## Future work
In the field of music generation, there are several crucial areas that warrant further exploration. Addressing the lack of comprehensive datasets and developing robust evaluation methods are key priorities to enhance the quality and variety of generated music. Additionally, extending the latent space optimization approach to other models would provide fine-grained control over music generation, enabling specific genre or style biases. Moreover, an intriguing avenue is the development of a text-to-music generative model based on the research conducted in this study, leveraging insights from latent space optimization and conditioning techniques to translate textual input into coherent and expressive musical compositions. By advancing in these directions, we can unlock new possibilities for creative expression and push the boundaries of generative music.

# Research no.2: copyright of all the future song lyrics
## Abstract
The exponential growth of storage and computing power over the past decade has posed challenges for copyright laws to keep pace with advancing technology. This paper addresses the issue of copyrightability of future song lyrics and accidental infringement of pop song lyrics. The potential exploitation of copyright laws by law firms to sue for royalties based on computing common future pop song lyrics is discussed. To prevent this, the paper proposes copyrighting all likely future song lyrics and thereby mitigating accidental infringement of existing pop songs.

## Copyright laws issue in the pop music industry
Copyright is a critical subject within the art industry. While song titles, rhythms, musical styles, and harmony are generally not copyrightable, melodies and lyrics enjoy legal protection. Originally, copyright laws aimed to safeguard artists who created easily reproducible artworks such as paintings, music compositions, and literature. However, in recent years, musicologists and anthropologists have highlighted the industrialization and commercialization of pop music. These researchers argue that compared to classical or jazz genres, pop songs tend to have less complex melodies, harmonies, and lyrics, with greater similarity among them. Conversely, other researchers assert that pop music complexity has shifted to aspects like music visual clips and sound effects, rendering traditional copyright laws inadequate in protecting pop music. \[[11]\]

## Related work
The issue of copyrighting melodies has been explored in a TED talk by Daniel Riehl. Riehl's approach involves brute-forcing all future common melodies of pop songs and storing them, constituting valid copyright claims for those melodies. By providing public access to these copyrights, Riehl aims to prevent accidental infringement.  \[[12]\]

## Approach
Utilizing a corpus of pop song lyrics spanning from 1950 to 2020, a Markov chain model is created to represent the probability of word transitions in pop songs. This model enables the computation of common combinations with a defined probability threshold, allowing estimation of the computational and storage requirements necessary to store and copyright all likely future lyrics.

## Implemetation
One of the challenges encountered was the scarcity of a pure English pop lyrics dataset. While most available datasets encompassed other genres and languages, a dataset primarily comprising English pop songs was selected for proof-of-concept purposes. Preprocessing techniques were applied to this dataset. \[[13]\] 
Subsequently, the dataset was used to train a maximum likelihood estimation (MLE)-based language model, specifically a simple Markov chain. This model tallies the frequency of word transitions. After training on the entire dataset, the number of common combinations exceeding a defined probability threshold is computed. Estimation of the required storage and computational power can be derived from the number of combinations and the storage capacity needed for each combination.

[code: https://github.com/turiPO/virtuosicAI/tree/main/CopyrightFuturePop]

## Future work
In a hypothetical scenario where all future melodies and lyrics can be stored on a disk, the relevance of copyright law becomes questionable. However, it is highly improbable that the current legal interpretation would align with such a notion. An alternative solution could involve developing a tool to assess the distinctiveness of a song in comparison to previously published pop songs. Such a tool could be utilized by record companies and artists to determine if a song meets the originality threshold for publication. Furthermore, the implementation of the Markov chain can be enhanced by leveraging more accurate neural language models.


# Reference
\[1\] "Gaëtan Hadjeres, François Pachet, Frank Nielsen. Carlos Hernandez-Olivan, Jose R. Beltran. Music Composition with Deep Learning: A Review, 2017." <br>
\[2\]  " DeepBach: a Steerable Model for Bach Chorales Generation, 2021." <br>
\[3\] "Cheng-Zhi Anna Huang, Tim Cooijmans, Adam Roberts, Aaron Courville, Douglas Eck. Counterpoint by Convolution, 2019." <br>
\[4\] "Adam Roberts, Jesse Engel, Colin Raffel, Curtis Hawthorne, Douglas Eck. A Hierarchical Latent Vector Model for Learning Long-Term Structure in Music, 2019." <br>
\[5\] "Vaswani, Ashish, Shazeer, Noam, Parmar, Niki, Uszkoreit, Jakob, Jones, Llion, Gomez, Aidan N, Kaiser, Łukasz and Polosukhin, Illia. Attention is all you need, 2017." <br>
\[6\] "Cheng-Zhi Anna Huang, Ashish Vaswani, Jakob Uszkoreit, Noam Shazeer. Music Transformer, 2018" <br>
\[7\] "Christine. MuseNet. OpenAI, 2019." <br>
\[8\] "Prafulla Dhariwal, Heewoo Jun, Christine Payne, Jong Wook Kim, Alec Radford, Ilya Sutskever. Jukebox: A Generative Model for Music, 2020." <br>
\[9\] "Aviv Gabbay, Yedid Hoshen. Style Generator Inversion for Image Enhancement and Animation, 2019." <br>
\[10\] "Piotr Bojanowski, Armand Joulin, David Lopez-Paz, Arthur Szlam. Optimizing the Latent Space of Generative Networks, 2018." <br>
\[11\]: "Adorno, Theodor W. and Horkheimer, Max. The Culture Industry: Enlightenment as Mass Deception. West Sussex: Columbia University Press, 2020, pp. 80-96" <br>
\[12\]: "Damien Riehl. Copyrighting all the melodies to avoid accidental infringement. 2020" <br>
\[13\]: "https://github.com/mathigatti/pop-lyrics-dataset/tree/master/lyrics" <br>

[1]: <https://doi.org/10.48550/arXiv.2108.12290> "Gaëtan Hadjeres, François Pachet, Frank Nielsen. Carlos Hernandez-Olivan, Jose R. Beltran. Music Composition with Deep Learning: A Review, 2017."
[2]: <https://doi.org/10.48550/arXiv.1612.01010> " DeepBach: a Steerable Model for Bach Chorales Generation, 2021."
[3]: <https://doi.org/10.48550/arXiv.1903.07227> "Cheng-Zhi Anna Huang, Tim Cooijmans, Adam Roberts, Aaron Courville, Douglas Eck. Counterpoint by Convolution, 2019."
[4]: <https://doi.org/10.48550/arXiv.1803.05428> "Adam Roberts, Jesse Engel, Colin Raffel, Curtis Hawthorne, Douglas Eck. A Hierarchical Latent Vector Model for Learning Long-Term Structure in Music, 2019."
[5]: <https://doi.org/10.48550/arXiv.1706.03762> "Vaswani, Ashish, Shazeer, Noam, Parmar, Niki, Uszkoreit, Jakob, Jones, Llion, Gomez, Aidan N, Kaiser, Łukasz and Polosukhin, Illia. Attention is all you need, 2017."
[6]: <https://doi.org/10.48550/arXiv.1809.04281> "Cheng-Zhi Anna Huang, Ashish Vaswani, Jakob Uszkoreit, Noam Shazeer. Music Transformer, 2018"
[7]: <https://openai.com/blog/musenet/> "Payne, Christine. MuseNet. OpenAI, 2019."
[8]: <https://doi.org/10.48550/arXiv.2005.00341> "Prafulla Dhariwal, Heewoo Jun, Christine Payne, Jong Wook Kim, Alec Radford, Ilya Sutskever. Jukebox: A Generative Model for Music, 2020."
[9]: <https://doi.org/10.48550/arXiv.1906.11880> "Aviv Gabbay, Yedid Hoshen. Style Generator Inversion for Image Enhancement and Animation, 2019."
[10]: <https://doi.org/10.48550/arXiv.1707.05776> "Piotr Bojanowski, Armand Joulin, David Lopez-Paz, Arthur Szlam. Optimizing the Latent Space of Generative Networks, 2018."
[11]: <https://doi.org/10.7312/kul-17602-005> "Adorno, Theodor W. and Horkheimer, Max. The Culture Industry: Enlightenment as Mass Deception. West Sussex: Columbia University Press, 2020, pp. 80-96"
[12]: <https://www.youtube.com/watch?v=sJtm0MoOgiU> "Damien Riehl. Copyrighting all the melodies to avoid accidental infringement. 2020"
[13]: <https://github.com/mathigatti/pop-lyrics-dataset/tree/master/lyrics> "https://github.com/mathigatti/pop-lyrics-dataset/tree/master/lyrics"

# Appendix - small experiments
During the work we conducted many experiment with different models and datasets. Here are some of them:
*Music Transformer longer melodies:* Music transformer is a large transformer model by Google Magenta which can produce melodies using a "prompt" - beginning of the melody. One of the main issues with Music Transformer is that it is very limited by the context it can preserved through time, means after a few minutes the music sounds like random. In order to overcome this limitation we tried to concat melodies by taking each melody ending as the prompt for the next one. The results where far better than the naive generation of long melody.
[code: https://github.com/turiPO/virtuosicAI/blob/main/notebooks/music%20transfomer.ipynb]
<br>
*MusicVAE with conditional varience:* Dropping the encoder of the VAE, we can inject a random vector into the decoder and create melodies. A regular VAE interpret the encoder output as mean and varience for normal gaussian sampling, which then feed the output of it into the decoder. In order to test MusicVAE we sampled from a normal distribution and injected it into the decoder. Each time we sampled with higher varience in order to validate the behaviour of the model. The results where as expected, the model generated more random melodies with higher varience and total random when the vector was randomed. 
[code: https://github.com/turiPO/virtuosicAI/blob/main/notebooks/MusicVAE%20notebooks.ipynb]
