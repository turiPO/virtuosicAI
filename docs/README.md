[TOC]
# Abstract
In this work we examine the ability of a machine to compose good quality music with minimalistic amount of resources. Beginning with relevant review of previous work and the current state of the art model we describe new approache that hasn't been done yet in the music geenration field.

# Music Generation - A Review
## Passing the Turing test

AI based music composition has been in research since 1980. Classic algorithms, methods from information theory, probabilistic models, chaos theory and genetics algorithms were mainly in use for this task. \[[1]\]

There has been a recent surge in the use of deep learning techniques for music generation. This is largely due to the success of these techniques in other domains such as computer vision and natural language processing. Deep learning offers a powerful tool for learning complex patterns in data. The first time music generated by deep learning (DL) passed the Turing test was the "Deep Bach" project in 2017, were they composed chorales music based on Bach chorales.

The Deep Bach project mixed classic algorithms as well Bidirectional recurrent neural network (RNN) model which has been trained by masking - prediction erased arpeggios in Bach chorales (randomly erased arpeggio and ask the model to reconstruct it). In addition, this work was revolutionised in two ways:
1. Similar to image augmentation, the researchers expanded the training dataset by augmenting the music scale of each chorale.
2. Chose the depth of the RNN according the average radius between chord and it's dependents.
\[[2]\]

Meanwhile Magenta project (Acquired by Google) worked on a similar task and publish in 2019. Instead of using recurrent NN (neural network) they used CNN (convolutional NN) to complete partial scores. In order to fix CNN lack of chronologicalization, after the generation the scores run by ML model which check the likelihood of the output based on previous chords and "fix" it to be more chronological. While RNN limited by it's short memory (vanishing/exploding gradient), this CNN based method is more efficient because it can run parallel and proved to be able to capture local structure as well as large scale structure. \[[3]\]

Despite the improvement,  chorales composition is an "easy task" because they are very patterned and been written under strict rules. Chorale is a music piece without melody or rhythm which contains four arpeggios in each bar, limited to four voices choir and written under strict harmony rules of the baroque era. Actually, when Bach didn't follow the harmony rules he had been severely criticised in the newspapers.

## Advanced models
The main challenge in the music generation task is to generate longer and better quality music pieces within different genres and thanks to the recent revolutions in NLP and computer vision it's started to happen also in the music field. Most of the music generation breakthroughs of the past years can be divide into two approaches: latent based models and language based model. Latent based models papers are based mainly on VAEs (e.g. MusicVAE, PianoTree, Jukebox), although there are quite successful ones with GANs (e.g. GANSynth, jazzGAN). On the contrasts, music traditionally has been though as a language model, and thus there are state of the art solutions based on transformers (e.g. Music Transformer, Musenet).

The first time VAE has succeeded to catch "long term structure" within 16 bars was MusicVAE, which is able to generate style diverse and good quality melodies. Unlike Vision works with VAE, in order to be able to choose piece length MusicVAE uses Bi-RNN as the decoder and encoder. MusicVAE trained on the Lakh MIDI Dataset and another 1.5 million MIDIs on the web without performance information. In addition, MusicVAE achieves good results when interpolates the latent space between two pieces. \[[4]\] 

Transformers based model were first introduced for NLP and were very successful. The main reason for the success was the introduction of the attention layer, which was able to learn the right relation between words in the sentence. Attention layer is ignorant to the order of the words in the sentence, therefore the writers of the paper found a way to encode the position of each word. Unlike RNNs, transformers can be trained with big amount of data, which is more accessible due to the development of the internet and managed to be trained unsupervised for NLP tasks. \[[5]\]

Magenta team were the first ones to adopt the transformers revolution into the music world  with "Music Transformer". Each "music sentence" is longer than natural language sentence, therefore the paper writers suggested an alternative attention layer algorithm which reduces the space complexity from squared into linear. Music Transformer has been trained on the traditional YAMAHA piano-e-competition dataset as well as all the Youtube piano videos. \[[6]\].

OpenAI created two transformer based models as well: first Musenet and then Jukebox. Musnet has similar architecture as GPT-2 and trained with masking as well: prediction of a next token in a sequence. Musenet has been trained on MIDIs datasets which where augmented for better results and for expansion of the amount of the training data. \[[7]\]

Jukebox is the current state of the art music generator, it can generate both melody and harmony and performance elements like lyrics, background noises and human sound. Jukebox dataset contains 1.2 milion songs which was crawled from the web with lyrics and song metadata. In comparison to most of the models which has been discussed in this paper, Jukebox is . \[[8]\]

<!---
## Other Developments

While automatic music generation 

Short pieces:
MusicVAE - https://magenta.tensorflow.org/music-vae
Music Trasformer - magenta
Musenet - GPT2 openai 

Longer pieces:
TransformerVAE
PianoTree
DDPM
Try to find structure with C-RBM.

## Generate melody given harmony 
VAE - Generating nontrivial melodies for music as a service 2017
[https://arxiv.org/pdf/1809.07600.pdf]
JazzGAN - Improvising with generative adversarial networks 2018
[https://musicalmetacreation.org/mume2018/proceedings/Trieu.pdf]
BebopNet - jazz improvisiations with LSTM 2020
[https://program.ismir2020.net/static/final_papers/132.pdf]

## Musical style transfer
Although the lack of datasets style transfer methods has been used such as tune transfer and instruments addition algorithms, VAE and transfer learning. [https://arxiv.org/pdf/2108.12290.pdf]
The first DNN which succeeded to transfer style of a complete music piece was MIDI-VAE in 2018.   [https://arxiv.org/pdf/1809.07600.pdf]
Transfer learning methods found as very effective for this task, <> find tuned pop generation model into urban music. [????]

Another effective method was by learning the PianoTree VAE model latent vector representation and modify it in a way which changes the style of the music. [https://arxiv.org/pdf/2008.07122.pdf]

GAN based method to mix many genres [https://arxiv.org/pdf/1712.01456.pdf]

## Challenges in music generation

### Evaluation of  the music

The music generation task is inherently unsupervised learning task.

* Brain EEG (technion jazz paper)
* chord progression histogram
* Train validators (for style transfer)
* Listening tests (like in MuseicVAE)
* creativity
  * Rote Memorization frequencies (RM): Given a specified
  length l, RM measures how frequently the model copies
  note sequences of length l from the corpus. [https://musicalmetacreation.org/mume2018/proceedings/Trieu.pdf]
  * Pitch variation [https://musicalmetacreation.org/mume2018/proceedings/Trieu.pdf]

## Feature Engineering

A music piece contains a few important elements, which used as feature in the generation papaers:

1. Key - which can be switched between parts of the piece
2. Rhythm
3. Structure - Sonata, perlude, ABABA @todo
4. Melody @todo
5. Texture - @todo
6. Instruments - voice amount, voices range, what kind of instrument etc.
-->
# Mini project: prior latent space optimization using MusicVAE 
<!-- During the research I explored various generation models and focus on two topics. --> 
## Introduction
Optimizing a model is an important task in the field of DL, it can be for better quiality purposes or for converting a model to do a more specific task. In case of music generation, many models can generate good quality music but without a way to produce a specific genre.   

One of the main problem with latent based music generation models is the unknown behavior of the latent space. Many times the style of the output is a style mixture of the training data or randomly picked style represents the training data in some way. In addition, fine tuning the whole network for each style or for each piece reconstruction is extremely inefficient and impossible under limited resources.

Computer vision works found a way to overcome this issue by creating less expressive neural network which study by KL-divergence loss the distribution of the latent space of a prior distribution. This enables generative models to be conditional, for example reconstruct a particular face with StyleGAN or interpolating between faces while setting a specific hair color \[[9]\] \[[10]\].

## Approach 
We used latent space optimization technique using MusiceVAE model in order to enable conditioning of a prior. In order to acheive that we can freeze the weights of the VAE decoder and train a new small version of the encoder (in the same way the training has been performed  in the original paper). The training data contains a specific genres of midi files, therefore the new encoder can be much smaller than the one in the original MusicVAE paper. This approach has been tested on MusicVAE but can very useful for any music generation latent based model. 

## Implementation
We developed the ability to fine-tune any part of MusicVAE 16 bars generation model, which means it can be trained under a specific genre and then it will be biased towards it. For research purposes we allowd couple of options: train only the decoder or train only the encoder or train both. Creating a smaller encoder is not needed for MusicVAE becasue the training is quick enaugh and will stay for future work.

[code: https://github.com/turiPO/virtuousicAI/tree/main/PriorMusicVAE]

<!--
# Mini project no.2: text2text2music transformers

@staytuned
-->

# Reference
\[1\] "Gaëtan Hadjeres, François Pachet, Frank Nielsen. Carlos Hernandez-Olivan, Jose R. Beltran. Music Composition with Deep Learning: A Review, 2017." <br>
\[2\]  " DeepBach: a Steerable Model for Bach Chorales Generation, 2021." <br>
\[3\] "Cheng-Zhi Anna Huang, Tim Cooijmans, Adam Roberts, Aaron Courville, Douglas Eck. Counterpoint by Convolution, 2019." <br>
\[4\] "Adam Roberts, Jesse Engel, Colin Raffel, Curtis Hawthorne, Douglas Eck. A Hierarchical Latent Vector Model for Learning Long-Term Structure in Music, 2019." <br>
\[5\] "Vaswani, Ashish, Shazeer, Noam, Parmar, Niki, Uszkoreit, Jakob, Jones, Llion, Gomez, Aidan N, Kaiser, Łukasz and Polosukhin, Illia. Attention is all you need, 2017." <br>
\[6\] "Cheng-Zhi Anna Huang, Ashish Vaswani, Jakob Uszkoreit, Noam Shazeer. Music Transformer, 2018" <br>
\[7\] "Christine. MuseNet. OpenAI, 2019." <br>
\[8\] "Prafulla Dhariwal, Heewoo Jun, Christine Payne, Jong Wook Kim, Alec Radford, Ilya Sutskever. Jukebox: A Generative Model for Music, 2020." <br>
\[9\] "Aviv Gabbay, Yedid Hoshen. Style Generator Inversion for Image Enhancement and Animation, 2019." <br>
\[10\] "Piotr Bojanowski, Armand Joulin, David Lopez-Paz, Arthur Szlam. Optimizing the Latent Space of Generative Networks, 2018." <br>

[1]: <https://doi.org/10.48550/arXiv.2108.12290> "Gaëtan Hadjeres, François Pachet, Frank Nielsen. Carlos Hernandez-Olivan, Jose R. Beltran. Music Composition with Deep Learning: A Review, 2017."
[2]: <https://doi.org/10.48550/arXiv.1612.01010> " DeepBach: a Steerable Model for Bach Chorales Generation, 2021."
[3]: <https://doi.org/10.48550/arXiv.1903.07227> "Cheng-Zhi Anna Huang, Tim Cooijmans, Adam Roberts, Aaron Courville, Douglas Eck. Counterpoint by Convolution, 2019."
[4]: <https://doi.org/10.48550/arXiv.1803.05428> "Adam Roberts, Jesse Engel, Colin Raffel, Curtis Hawthorne, Douglas Eck. A Hierarchical Latent Vector Model for Learning Long-Term Structure in Music, 2019."
[5]: <https://doi.org/10.48550/arXiv.1706.03762> "Vaswani, Ashish, Shazeer, Noam, Parmar, Niki, Uszkoreit, Jakob, Jones, Llion, Gomez, Aidan N, Kaiser, Łukasz and Polosukhin, Illia. Attention is all you need, 2017."
[6]: <https://doi.org/10.48550/arXiv.1809.04281> "Cheng-Zhi Anna Huang, Ashish Vaswani, Jakob Uszkoreit, Noam Shazeer. Music Transformer, 2018"
[7]: <https://openai.com/blog/musenet/> "Payne, Christine. MuseNet. OpenAI, 2019."
[8]: <https://doi.org/10.48550/arXiv.2005.00341> "Prafulla Dhariwal, Heewoo Jun, Christine Payne, Jong Wook Kim, Alec Radford, Ilya Sutskever. Jukebox: A Generative Model for Music, 2020."
[9]: <https://doi.org/10.48550/arXiv.1906.11880> "Aviv Gabbay, Yedid Hoshen. Style Generator Inversion for Image Enhancement and Animation, 2019."
[10]: <https://doi.org/10.48550/arXiv.1707.05776> "Piotr Bojanowski, Armand Joulin, David Lopez-Paz, Arthur Szlam. Optimizing the Latent Space of Generative Networks, 2018."