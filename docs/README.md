[TOC]
# Abstract
In this work we examine the ability of a machine to compose good quality music with minimalistic amount of resources. Beginning with relevant review of previous work and the current state of the art model we describe new approache that hasn't been done yet in the music geenration field.

# Music Generation - A Review
## Passing the Turing test

AI based music composition has been in research since 1980. Classic algorithms, methods from information theory, probabilistic models, chaos theory and genetics algorithms were mainly in use for this task. \[[1]\]

There has been a recent surge in the use of deep learning techniques for music generation. This is largely due to the success of these techniques in other domains such as computer vision and natural language processing. Deep learning offers a powerful tool for learning complex patterns in data. The first time music generated by deep learning (DL) passed the Turing test was the "Deep Bach" project in 2017, were they composed chorales music based on Bach chorales.

The Deep Bach project mixed classic algorithms as well Bidirectional recurrent neural network (RNN) model which has been trained by masking - prediction erased arpeggios in Bach chorales (randomly erased arpeggio and ask the model to reconstruct it). In addition, this work was revolutionised in two ways:
1. Similar to image augmentation, the researchers expanded the training dataset by augmenting the music scale of each chorale.
2. Chose the depth of the RNN according the average radius between chord and it's dependents.
\[[2]\]

Meanwhile Magenta project (Acquired by Google) worked on a similar task and publish in 2019. Instead of using recurrent NN (neural network) they used CNN (convolutional NN) to complete partial scores. In order to fix CNN lack of chronologicalization, after the generation the scores run by ML model which check the likelihood of the output based on previous chords and "fix" it to be more chronological. While RNN limited by it's short memory (vanishing/exploding gradient), this CNN based method is more efficient because it can run parallel and proved to be able to capture local structure as well as large scale structure. \[[3]\]

Despite the improvement,  chorales composition is an "easy task" because they are very patterned and been written under strict rules. Chorale is a music piece without melody or rhythm which contains four arpeggios in each bar, limited to four voices choir and written under strict harmony rules of the baroque era. Actually, when Bach didn't follow the harmony rules he had been severely criticised in the newspapers.

## Advanced models
The main challenge in the music generation task is to generate longer and better quality music pieces within different genres and thanks to the recent revolutions in NLP and computer vision it's started to happen also in the music field. Most of the music generation breakthroughs of the past years can be divide into two approaches: latent based models and language based model. Latent based models papers are based mainly on VAEs (e.g. MusicVAE, PianoTree, Jukebox), although there are quite successful ones with GANs (e.g. GANSynth, jazzGAN). On the contrasts, music traditionally has been though as a language model, and thus there are state of the art solutions based on transformers (e.g. Music Transformer, Musenet).

The first time VAE has succeeded to catch "long term structure" within 16 bars was MusicVAE, which is able to generate style diverse and good quality melodies. Unlike Vision works with VAE, in order to be able to choose piece length MusicVAE uses Bi-RNN as the decoder and encoder. MusicVAE trained on the Lakh MIDI Dataset and another 1.5 million MIDIs on the web without performance information. In addition, MusicVAE achieves good results when interpolates the latent space between two pieces. \[[4]\] 

Transformers based model were first introduced for NLP and were very successful. The main reason for the success was the introduction of the attention layer, which was able to learn the right relation between words in the sentence. Attention layer is ignorant to the order of the words in the sentence, therefore the writers of the paper found a way to encode the position of each word. Unlike RNNs, transformers can be trained with big amount of data, which is more accessible due to the development of the internet and managed to be trained unsupervised for NLP tasks. \[[5]\]

Magenta team were the first ones to adopt the transformers revolution into the music world  with "Music Transformer". Each "music sentence" is longer than natural language sentence, therefore the paper writers suggested an alternative attention layer algorithm which reduces the space complexity from squared into linear. Music Transformer has been trained on the traditional YAMAHA piano-e-competition dataset as well as all the Youtube piano videos. \[[6]\].

OpenAI created two transformer based models as well: first Musenet and then Jukebox. Musnet has similar architecture as GPT-2 and trained with masking as well: prediction of a next token in a sequence. Musenet has been trained on MIDIs datasets which where augmented for better results and for expansion of the amount of the training data. \[[7]\]

Jukebox is the current state of the art music generator, it can generate both melody and harmony and performance elements like lyrics, background noises and human sound. Jukebox dataset contains 1.2 milion songs which was crawled from the web with lyrics and song metadata. In comparison to most of the models which has been discussed in this paper, Jukebox is . \[[8]\]

<!---
## Other Developments

While automatic music generation 

Short pieces:
MusicVAE - https://magenta.tensorflow.org/music-vae
Music Trasformer - magenta
Musenet - GPT2 openai 

Longer pieces:
TransformerVAE
PianoTree
DDPM
Try to find structure with C-RBM.

## Generate melody given harmony 
VAE - Generating nontrivial melodies for music as a service 2017
[https://arxiv.org/pdf/1809.07600.pdf]
JazzGAN - Improvising with generative adversarial networks 2018
[https://musicalmetacreation.org/mume2018/proceedings/Trieu.pdf]
BebopNet - jazz improvisiations with LSTM 2020
[https://program.ismir2020.net/static/final_papers/132.pdf]

## Musical style transfer
Although the lack of datasets style transfer methods has been used such as tune transfer and instruments addition algorithms, VAE and transfer learning. [https://arxiv.org/pdf/2108.12290.pdf]
The first DNN which succeeded to transfer style of a complete music piece was MIDI-VAE in 2018.   [https://arxiv.org/pdf/1809.07600.pdf]
Transfer learning methods found as very effective for this task, <> find tuned pop generation model into urban music. [????]

Another effective method was by learning the PianoTree VAE model latent vector representation and modify it in a way which changes the style of the music. [https://arxiv.org/pdf/2008.07122.pdf]

GAN based method to mix many genres [https://arxiv.org/pdf/1712.01456.pdf]

## Challenges in music generation

### Evaluation of  the music

The music generation task is inherently unsupervised learning task.

* Brain EEG (technion jazz paper)
* chord progression histogram
* Train validators (for style transfer)
* Listening tests (like in MuseicVAE)
* creativity
  * Rote Memorization frequencies (RM): Given a specified
  length l, RM measures how frequently the model copies
  note sequences of length l from the corpus. [https://musicalmetacreation.org/mume2018/proceedings/Trieu.pdf]
  * Pitch variation [https://musicalmetacreation.org/mume2018/proceedings/Trieu.pdf]

## Feature Engineering

A music piece contains a few important elements, which used as feature in the generation papaers:

1. Key - which can be switched between parts of the piece
2. Rhythm
3. Structure - Sonata, perlude, ABABA @todo
4. Melody @todo
5. Texture - @todo
6. Instruments - voice amount, voices range, what kind of instrument etc.
-->
# Research no.1: prior latent space optimization using MusicVAE 
<!-- During the research I explored various generation models and focus on two topics. --> 
## Introduction
Optimizing a model is an important task in the field of DL, it can be for better quiality purposes or for converting a model to do a more specific task. In case of music generation, many models can generate good quality music but without a way to produce a specific genre.   

One of the main problem with latent based music generation models is the unknown behavior of the latent space. Many times the style of the output is a style mixture of the training data or randomly picked style represents the training data in some way. In addition, fine tuning the whole network for each style or for each piece reconstruction is extremely inefficient and impossible under limited resources.

Computer vision works found a way to overcome this issue by creating less expressive neural network which study by KL-divergence loss the distribution of the latent space of a prior distribution. This enables generative models to be conditional, for example reconstruct a particular face with StyleGAN or interpolating between faces while setting a specific hair color \[[9]\] \[[10]\].

## Approach 
We used latent space optimization technique using MusiceVAE model in order to enable conditioning of a prior. In order to acheive that we can freeze the weights of the VAE decoder and train a new small version of the encoder (in the same way the training has been performed  in the original paper). The training data contains a specific genres of midi files, therefore the new encoder can be much smaller than the one in the original MusicVAE paper. This approach has been tested on MusicVAE but can very useful for any music generation latent based model. 

## Implementation
We developed the ability to fine-tune any part of MusicVAE 16 bars generation model, which means it can be trained under a specific genre and then it will be biased towards it. For research purposes we allowd couple of options: train only the decoder or train only the encoder or train both. Creating a smaller encoder is not needed for MusicVAE becasue the training is quick enaugh and will stay for future work.

[code: https://github.com/turiPO/virtuousicAI/tree/main/PriorMusicVAE]

## Future work
There are many unwalked areas in the field of music generation. The most important one is the lack of datasets, which is the main reason why the models are not as good as they should be. In addition, the lack of evaluation methods is a big problem, because the current mathematical developement to measure music is very basic. The latent space optimization project can be extended in many ways, for example by converting other latent based music generation models into a conditional ones. Another way is to create a new evaluation method which will be used for training.

# Research no.2: copyright of all the future song lyrics
## Abstract
The growth of storage and computing power is exponential in the past decade. However, sometimes the law is not able to keep up with the technology. In this paper we will discuss ability to the copyright of all she future song lyrics and accidental infringement of pop songs lyrics.
Theoretically, by computing all the common future pop song lyrics a law firm can abuse the copyright law ans sue royalties. The paper prevents this issue by copyrighting all the future song lyrics which are likely to be. In addition, this way it prevents accidental infringement of pop songs lyrics.

## Copyright laws issue in the pop music industry
Copyright is one of the main topics in the art insdusry. While songs titles, rhythms, musical styles and harmony have all been doomed to be not copyrightable, melodies and lyrics are protected. Copyrights laws originally protects artists who creates artworks which are easy to copy (e.g. painting, music piece, litrature). However, in the past decade many musicologists and anthropologists have pointed to the indutrialisation and capitalization of the pop music. According to these researcher, in today's pop music industry the melody and harmony is much less complex then other classical genres, the pop songs are more similiar to one another comparing to classical music or jazz music. On the other side, other researchers says pop music complication moved from the harmony, melody and lyrics to other aspect of the music like the music visual clip and the sound effects. Therefore, the copyright laws, which protects only lyrics and melody are not relevant anymore. \[[11]\]

## Related work
Copyright issue with melodies has been disccussed in a TED talk by Daniel Riehl. Riehl bruteforces all the future common melodies of pop songs and store it on a disk, which counts as a valid copyright of the melodies. By providing the copyright to the public Rihel claims to prevent accidental infringement.  \[[12]\]

## Approach
Using the corpus of all the lyrics of the pop songs from 1950 to 2020 we can create a markov chain contains the probabilities of the transitions between one word to another in pop songs. Then we compute the amount of common combinations with certain probability threshold and estimate the amount of compute and storage power needed in order to store all the common future lyrics and thus copyright them.

## Implemetation
One of the challenges is the lack of pure english pop lyrics dataset. Most of the dataset contains other genres and other language. However, for the POC purposes we took a dataset which most of it contains english pop songd and we preprocess it. \[[13]\] 
After the pre-processing we used the dataset in order to train a language model based on MLE. The model is a simple markov chain which counts the amount of times a word is followed by another word. After training the model on the whole dataset we compute the amount of common combinations with certain probability threshold. The amount of storage and compute power needed can be estimated by the amount of combinations and the amount of bytes needed to store each combination.

[code: https://github.com/turiPO/virtuosicAI/tree/main/CopyrightFuturePop]

## Future work
In a world in which all the future melodies and lyrics can be stored on a disk the copyright law is theoretically not relevant. However, it is highly unlikely that the supreme court will stick to the current interpretation of the law. A possible solution is to create a tool which can check how distinugish a song from pop songs published before. This tool can be used by the record companies and artists in order to check if their song is original enough to be published.
In addition, the implementation of the markov chain can be improved by using neural language models which are more accurate.


# Reference
\[1\] "Gaëtan Hadjeres, François Pachet, Frank Nielsen. Carlos Hernandez-Olivan, Jose R. Beltran. Music Composition with Deep Learning: A Review, 2017." <br>
\[2\]  " DeepBach: a Steerable Model for Bach Chorales Generation, 2021." <br>
\[3\] "Cheng-Zhi Anna Huang, Tim Cooijmans, Adam Roberts, Aaron Courville, Douglas Eck. Counterpoint by Convolution, 2019." <br>
\[4\] "Adam Roberts, Jesse Engel, Colin Raffel, Curtis Hawthorne, Douglas Eck. A Hierarchical Latent Vector Model for Learning Long-Term Structure in Music, 2019." <br>
\[5\] "Vaswani, Ashish, Shazeer, Noam, Parmar, Niki, Uszkoreit, Jakob, Jones, Llion, Gomez, Aidan N, Kaiser, Łukasz and Polosukhin, Illia. Attention is all you need, 2017." <br>
\[6\] "Cheng-Zhi Anna Huang, Ashish Vaswani, Jakob Uszkoreit, Noam Shazeer. Music Transformer, 2018" <br>
\[7\] "Christine. MuseNet. OpenAI, 2019." <br>
\[8\] "Prafulla Dhariwal, Heewoo Jun, Christine Payne, Jong Wook Kim, Alec Radford, Ilya Sutskever. Jukebox: A Generative Model for Music, 2020." <br>
\[9\] "Aviv Gabbay, Yedid Hoshen. Style Generator Inversion for Image Enhancement and Animation, 2019." <br>
\[10\] "Piotr Bojanowski, Armand Joulin, David Lopez-Paz, Arthur Szlam. Optimizing the Latent Space of Generative Networks, 2018." <br>
\[11\]: "Adorno, Theodor W. and Horkheimer, Max. The Culture Industry: Enlightenment as Mass Deception. West Sussex: Columbia University Press, 2020, pp. 80-96" <br>
\[12\]: "Damien Riehl. Copyrighting all the melodies to avoid accidental infringement. 2020" <br>
\[13\]: "https://github.com/mathigatti/pop-lyrics-dataset/tree/master/lyrics" <br>

[1]: <https://doi.org/10.48550/arXiv.2108.12290> "Gaëtan Hadjeres, François Pachet, Frank Nielsen. Carlos Hernandez-Olivan, Jose R. Beltran. Music Composition with Deep Learning: A Review, 2017."
[2]: <https://doi.org/10.48550/arXiv.1612.01010> " DeepBach: a Steerable Model for Bach Chorales Generation, 2021."
[3]: <https://doi.org/10.48550/arXiv.1903.07227> "Cheng-Zhi Anna Huang, Tim Cooijmans, Adam Roberts, Aaron Courville, Douglas Eck. Counterpoint by Convolution, 2019."
[4]: <https://doi.org/10.48550/arXiv.1803.05428> "Adam Roberts, Jesse Engel, Colin Raffel, Curtis Hawthorne, Douglas Eck. A Hierarchical Latent Vector Model for Learning Long-Term Structure in Music, 2019."
[5]: <https://doi.org/10.48550/arXiv.1706.03762> "Vaswani, Ashish, Shazeer, Noam, Parmar, Niki, Uszkoreit, Jakob, Jones, Llion, Gomez, Aidan N, Kaiser, Łukasz and Polosukhin, Illia. Attention is all you need, 2017."
[6]: <https://doi.org/10.48550/arXiv.1809.04281> "Cheng-Zhi Anna Huang, Ashish Vaswani, Jakob Uszkoreit, Noam Shazeer. Music Transformer, 2018"
[7]: <https://openai.com/blog/musenet/> "Payne, Christine. MuseNet. OpenAI, 2019."
[8]: <https://doi.org/10.48550/arXiv.2005.00341> "Prafulla Dhariwal, Heewoo Jun, Christine Payne, Jong Wook Kim, Alec Radford, Ilya Sutskever. Jukebox: A Generative Model for Music, 2020."
[9]: <https://doi.org/10.48550/arXiv.1906.11880> "Aviv Gabbay, Yedid Hoshen. Style Generator Inversion for Image Enhancement and Animation, 2019."
[10]: <https://doi.org/10.48550/arXiv.1707.05776> "Piotr Bojanowski, Armand Joulin, David Lopez-Paz, Arthur Szlam. Optimizing the Latent Space of Generative Networks, 2018."
[11]: <https://doi.org/10.7312/kul-17602-005> "Adorno, Theodor W. and Horkheimer, Max. The Culture Industry: Enlightenment as Mass Deception. West Sussex: Columbia University Press, 2020, pp. 80-96"
[12]: <https://www.youtube.com/watch?v=sJtm0MoOgiU> "Damien Riehl. Copyrighting all the melodies to avoid accidental infringement. 2020"
[13]: <https://github.com/mathigatti/pop-lyrics-dataset/tree/master/lyrics> "https://github.com/mathigatti/pop-lyrics-dataset/tree/master/lyrics"

# Appendix - small experiments
During the work we conducted many experiment with different models and datasets. Here are some of them:
*Music Transformer longer melodies:* Music transformer is a large transformer model by Google Magenta which can produce melodies using a "prompt" - beginning of the melody. One of the main issues with Music Transformer is that it is very limited by the context it can preserved through time, means after a few minutes the music sounds like random. In order to overcome this limitation we tried to concat melodies by taking each melody ending as the prompt for the next one. The results where far better than the naive generation of long melody.
[code: https://github.com/turiPO/virtuosicAI/blob/main/notebooks/music%20transfomer.ipynb]
<br>
*MusicVAE with conditional varience:* Dropping the encoder of the VAE, we can inject a random vector into the decoder and create melodies. A regular VAE interpret the encoder output as mean and varience for normal gaussian sampling, which then feed the output of it into the decoder. In order to test MusicVAE we sampled from a normal distribution and injected it into the decoder. Each time we sampled with higher varience in order to validate the behaviour of the model. The results where as expected, the model generated more random melodies with higher varience and total random when the vector was randomed. 
[code: https://github.com/turiPO/virtuosicAI/blob/main/notebooks/MusicVAE%20notebooks.ipynb]